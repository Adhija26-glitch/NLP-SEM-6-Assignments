{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl1RXmZCbfr2",
        "outputId": "9ddf4ff5-36af-47ec-a9e7-e0c82259ef9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "!pip install nltk\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# NLP PREPROCESSING USING NLTK\n",
        "\n",
        "\n",
        "# Import tokenizers\n",
        "from nltk.tokenize import (\n",
        "    WhitespaceTokenizer,\n",
        "    wordpunct_tokenize,\n",
        "    TreebankWordTokenizer,\n",
        "    TweetTokenizer,\n",
        "    MWETokenizer\n",
        ")\n",
        "\n",
        "# Import stemmers and lemmatizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "# Sample text\n",
        "text = \"NLTK is a powerful library for NLP. I'm learning tokenization, stemming & lemmatization! #AI #NLP\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 1️⃣ Whitespace Tokenization\n",
        "wt = WhitespaceTokenizer()\n",
        "print(\"1. Whitespace Tokenization:\")\n",
        "print(wt.tokenize(text))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2️⃣ Punctuation-based Tokenization\n",
        "print(\"2. Punctuation-based Tokenization:\")\n",
        "print(wordpunct_tokenize(text))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3️⃣ Treebank Tokenization\n",
        "tb = TreebankWordTokenizer()\n",
        "print(\"3. Treebank Tokenization:\")\n",
        "print(tb.tokenize(text))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4️⃣ Tweet Tokenization\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(\"4. Tweet Tokenization:\")\n",
        "print(tweet_tokenizer.tokenize(text))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 5️⃣ MWE Tokenization\n",
        "mwe = MWETokenizer([('machine', 'learning'), ('natural', 'language', 'processing')])\n",
        "sentence = \"I am studying machine learning and natural language processing\"\n",
        "print(\"5. MWE Tokenization:\")\n",
        "print(mwe.tokenize(sentence.split()))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# STEMMING\n",
        "\n",
        "\n",
        "words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairness\"]\n",
        "\n",
        "# Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "print(\"Porter Stemming:\")\n",
        "for word in words:\n",
        "    print(word, \"->\", porter.stem(word))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Snowball Stemmer\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "print(\"Snowball Stemming:\")\n",
        "for word in words:\n",
        "    print(word, \"->\", snowball.stem(word))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# LEMMATIZATION\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"Lemmatization:\")\n",
        "print(\"running (verb) ->\", lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
        "print(\"better (adjective) ->\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuPPXtM_bwdC",
        "outputId": "d6a91d7b-bfcb-48a7-ff9d-98e824225d11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "NLTK is a powerful library for NLP. I'm learning tokenization, stemming & lemmatization! #AI #NLP\n",
            "--------------------------------------------------\n",
            "1. Whitespace Tokenization:\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP.', \"I'm\", 'learning', 'tokenization,', 'stemming', '&', 'lemmatization!', '#AI', '#NLP']\n",
            "--------------------------------------------------\n",
            "2. Punctuation-based Tokenization:\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '.', 'I', \"'\", 'm', 'learning', 'tokenization', ',', 'stemming', '&', 'lemmatization', '!', '#', 'AI', '#', 'NLP']\n",
            "--------------------------------------------------\n",
            "3. Treebank Tokenization:\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP.', 'I', \"'m\", 'learning', 'tokenization', ',', 'stemming', '&', 'lemmatization', '!', '#', 'AI', '#', 'NLP']\n",
            "--------------------------------------------------\n",
            "4. Tweet Tokenization:\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '.', \"I'm\", 'learning', 'tokenization', ',', 'stemming', '&', 'lemmatization', '!', '#AI', '#NLP']\n",
            "--------------------------------------------------\n",
            "5. MWE Tokenization:\n",
            "['I', 'am', 'studying', 'machine_learning', 'and', 'natural_language_processing']\n",
            "--------------------------------------------------\n",
            "Porter Stemming:\n",
            "running -> run\n",
            "runs -> run\n",
            "runner -> runner\n",
            "easily -> easili\n",
            "fairness -> fair\n",
            "--------------------------------------------------\n",
            "Snowball Stemming:\n",
            "running -> run\n",
            "runs -> run\n",
            "runner -> runner\n",
            "easily -> easili\n",
            "fairness -> fair\n",
            "--------------------------------------------------\n",
            "Lemmatization:\n",
            "running (verb) -> run\n",
            "better (adjective) -> good\n"
          ]
        }
      ]
    }
  ]
}