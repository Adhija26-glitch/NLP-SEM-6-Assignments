{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qig7Ahp0jiQr",
        "outputId": "6cd21a0e-d91a-48ee-9d6c-41de428e9fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk gensim scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FptcxADgjw8B",
        "outputId": "25a05bb2-8e04-4013-dd75-d64d2d523c64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Natural language processing is an important field According to me \",\n",
        "    \"Machine learning is used in natural language processing\",\n",
        "    \"Word embeddings capture semantic meaning\"\n",
        "    \"Adhija did this\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "4UmqxBujj-b9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "bow_count = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "bow_count_df = pd.DataFrame(\n",
        "    bow_count.toarray(),\n",
        "    columns=count_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"Bag of Words (Count Occurrence)\")\n",
        "print(bow_count_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7xOcLfikItk",
        "outputId": "3d7da9e1-9060-462e-beb4-cf2b14d7b531"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (Count Occurrence)\n",
            "   according  an  capture  did  embeddings  field  important  in  is  \\\n",
            "0          1   1        0    0           0      1          1   0   1   \n",
            "1          0   0        0    0           0      0          0   1   1   \n",
            "2          0   0        1    1           1      0          0   0   0   \n",
            "\n",
            "   language  ...  machine  me  meaningadhija  natural  processing  semantic  \\\n",
            "0         1  ...        0   1              0        1           1         0   \n",
            "1         1  ...        1   0              0        1           1         0   \n",
            "2         0  ...        0   0              1        0           0         1   \n",
            "\n",
            "   this  to  used  word  \n",
            "0     0   1     0     0  \n",
            "1     0   0     1     0  \n",
            "2     1   0     0     1  \n",
            "\n",
            "[3 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer_for_normalization = CountVectorizer()\n",
        "bow_counts_raw = count_vectorizer_for_normalization.fit_transform(documents)\n",
        "\n",
        "feature_names = count_vectorizer_for_normalization.get_feature_names_out()\n",
        "\n",
        "bow_counts_array = bow_counts_raw.toarray()\n",
        "\n",
        "row_sums = bow_counts_array.sum(axis=1, keepdims=True)\n",
        "row_sums[row_sums == 0] = 1  # Avoid division by zero for empty documents\n",
        "\n",
        "bow_normalized_array = bow_counts_array / row_sums\n",
        "\n",
        "bow_normalized_df = pd.DataFrame(\n",
        "    bow_normalized_array,\n",
        "    columns=feature_names\n",
        ")\n",
        "\n",
        "print(\"Bag of Words (Normalized Count)\")\n",
        "print(bow_normalized_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzOkf3WvklJO",
        "outputId": "6f343f45-7a6e-423c-d1dd-b67fd1f62d39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (Normalized Count)\n",
            "   according   an   capture       did  embeddings  field  important     in  \\\n",
            "0        0.1  0.1  0.000000  0.000000    0.000000    0.1        0.1  0.000   \n",
            "1        0.0  0.0  0.000000  0.000000    0.000000    0.0        0.0  0.125   \n",
            "2        0.0  0.0  0.142857  0.142857    0.142857    0.0        0.0  0.000   \n",
            "\n",
            "      is  language  ...  machine   me  meaningadhija  natural  processing  \\\n",
            "0  0.100     0.100  ...    0.000  0.1       0.000000    0.100       0.100   \n",
            "1  0.125     0.125  ...    0.125  0.0       0.000000    0.125       0.125   \n",
            "2  0.000     0.000  ...    0.000  0.0       0.142857    0.000       0.000   \n",
            "\n",
            "   semantic      this   to   used      word  \n",
            "0  0.000000  0.000000  0.1  0.000  0.000000  \n",
            "1  0.000000  0.000000  0.0  0.125  0.000000  \n",
            "2  0.142857  0.142857  0.0  0.000  0.142857  \n",
            "\n",
            "[3 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"TF-IDF Representation\")\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prcihF2Vks5w",
        "outputId": "ca6702e4-7883-4ce0-cd3a-0be434f95e7d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Representation\n",
            "   according        an   capture       did  embeddings     field  important  \\\n",
            "0   0.346821  0.346821  0.000000  0.000000    0.000000  0.346821   0.346821   \n",
            "1   0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   0.000000   \n",
            "2   0.000000  0.000000  0.377964  0.377964    0.377964  0.000000   0.000000   \n",
            "\n",
            "        in        is  language  ...  machine        me  meaningadhija  \\\n",
            "0  0.00000  0.263766  0.263766  ...  0.00000  0.346821       0.000000   \n",
            "1  0.39798  0.302674  0.302674  ...  0.39798  0.000000       0.000000   \n",
            "2  0.00000  0.000000  0.000000  ...  0.00000  0.000000       0.377964   \n",
            "\n",
            "    natural  processing  semantic      this        to     used      word  \n",
            "0  0.263766    0.263766  0.000000  0.000000  0.346821  0.00000  0.000000  \n",
            "1  0.302674    0.302674  0.000000  0.000000  0.000000  0.39798  0.000000  \n",
            "2  0.000000    0.000000  0.377964  0.377964  0.000000  0.00000  0.377964  \n",
            "\n",
            "[3 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=tokenized_docs,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "print(\"Word2Vec Embedding for 'language'\")\n",
        "print(word2vec_model.wv[\"language\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R3qQNzJkxyA",
        "outputId": "8da335e2-743c-494c-d6e7-bdf54c068a1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Embedding for 'language'\n",
            "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
            "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
            " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
            "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
            "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
            " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
            "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
            " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
            " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
            " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
            " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
            "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
            " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
            " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
            " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
            "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
            " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
            " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
            "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
            "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
            "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
            "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
            "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
            "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
            "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PVBW4ldk-un"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}